{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97837b83-494e-4645-8056-09b5750e1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "force_rerun = False\n",
    "run_once = False # useful for debugging, just runs one sample\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# code for enabling this notebook to work within cursor\n",
    "coralme_dir = '/home/chris/zuniga/coralme/' #'../'\n",
    "sys.path.insert(0, coralme_dir)\n",
    "\n",
    "#import cobra\n",
    "import coralme\n",
    "import coralme.solver.solver\n",
    "import coralme.builder.main\n",
    "import coralme.core.model\n",
    "import pickle\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "from adjustText import adjust_text\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def get_n_colors_from_colormap(n, colormap_name='viridis'):\n",
    "    cmap = cm.get_cmap(colormap_name)\n",
    "    if n == 1:\n",
    "        return(cmap(1))\n",
    "    else:\n",
    "        return [cmap(i / (n - 1)) for i in range(n)]\n",
    "    \n",
    "def create_m_to_me_mapping(m_model, me_model):\n",
    "    \"\"\"\n",
    "    Create a dictionary mapping M model reaction IDs to their corresponding ME model reaction IDs\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    m_model : cobra.Model\n",
    "        Original M model\n",
    "    me_model : coralme.core.MEModel\n",
    "        ME model generated from M model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with M model reaction IDs as keys and lists of corresponding ME reaction IDs as values\n",
    "    \"\"\"\n",
    "    m_to_me = {}\n",
    "    \n",
    "    # For each M model reaction, find corresponding ME reactions\n",
    "    for m_rxn in m_model.reactions:\n",
    "        # Find all ME reactions that contain this M reaction ID\n",
    "        me_rxns = [me_rxn.id for me_rxn in me_model.reactions \n",
    "                  if m_rxn.id in me_rxn.id]\n",
    "        \n",
    "        if me_rxns:\n",
    "            m_to_me[m_rxn.id] = me_rxns\n",
    "            \n",
    "    return m_to_me\n",
    "\n",
    "# build pathway reference using other models\n",
    "files = []\n",
    "for f in os.listdir(os.path.join(coralme_dir, 'data', 'external', 'pathways')):\n",
    "    path = os.path.join(coralme_dir, 'data', 'external', 'pathways', f)\n",
    "    files.append(path)\n",
    "files.sort()\n",
    "models_source_df = pd.read_excel(files[0], index_col = 0)\n",
    "models_source_df['source'] = 'iML1515'\n",
    "for f in files[1:]:\n",
    "    df = pd.read_excel(f, index_col = 0)\n",
    "    df['source'] = f.split('pathways/')[1][2:-4]\n",
    "    new = set(df.index) - set(models_source_df.index)\n",
    "    models_source_df = pd.concat([models_source_df, df.loc[list(new)]])\n",
    "\n",
    "# build pathway reference using Carlos mapping\n",
    "models_carlos_df = pd.read_csv(os.path.join(coralme_dir, 'data', 'external', 'geneIDsMapping_PA14_vs_Strains_SubsystemInfo.csv'), index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700238dd-3739-4d92-bba9-c407df03ba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP008896 : pathway annotated model already created, skipping\n",
      "CP053697 : pathway annotated model already created, skipping\n",
      "CP026386 : pathway annotated model already created, skipping\n",
      "CP069317 : pathway annotated model already created, skipping\n",
      "AE004091.2 : pathway annotated model already created, skipping\n",
      "CP014784 : pathway annotated model already created, skipping\n",
      "CP073105 : pathway annotated model already created, skipping\n",
      "CP065866 : pathway annotated model already created, skipping\n",
      "CP061848 : pathway annotated model already created, skipping\n",
      "CP012831 : pathway annotated model already created, skipping\n",
      "CP008749.1 : pathway annotated model already created, skipping\n",
      "CP032419 : pathway annotated model already created, skipping\n",
      "CP038001 : pathway annotated model already created, skipping\n",
      "CP022560 : pathway annotated model already created, skipping\n",
      "CP076683 : pathway annotated model already created, skipping\n",
      "CP045416 : pathway annotated model already created, skipping\n",
      "AP022324 : pathway annotated model already created, skipping\n",
      "LS483372 : pathway annotated model already created, skipping\n",
      "CP022562 : pathway annotated model already created, skipping\n",
      "LR590473 : pathway annotated model already created, skipping\n",
      "CP065865 : pathway annotated model already created, skipping\n",
      "CP058975 : pathway annotated model already created, skipping\n",
      "CP012830 : pathway annotated model already created, skipping\n",
      "AP014655.1 : pathway annotated model already created, skipping\n",
      "CP061335 : pathway annotated model already created, skipping\n",
      "CP068238 : pathway annotated model already created, skipping\n",
      "CP050291 : pathway annotated model already created, skipping\n",
      "CP070982 : pathway annotated model already created, skipping\n",
      "CP000712 : pathway annotated model already created, skipping\n",
      "AP024503 : pathway annotated model already created, skipping\n",
      "CP045349 : pathway annotated model already created, skipping\n",
      "CP063780 : pathway annotated model already created, skipping\n",
      "CP024478 : pathway annotated model already created, skipping\n",
      "CP022561 : pathway annotated model already created, skipping\n",
      "CP065867 : pathway annotated model already created, skipping\n",
      "CP020100 : pathway annotated model already created, skipping\n",
      "CP039749 : pathway annotated model already created, skipping\n",
      "CP041013 : pathway annotated model already created, skipping\n",
      "CP015225 : pathway annotated model already created, skipping\n",
      "CP045359 : pathway annotated model already created, skipping\n",
      "CP043320 : pathway annotated model already created, skipping\n",
      "CP060288 : pathway annotated model already created, skipping\n",
      "CP043179 : pathway annotated model already created, skipping\n"
     ]
    }
   ],
   "source": [
    "# create input files for later\n",
    "\n",
    "# look for default starting media and annotate pathways\n",
    "if False: # this is no longer necessary\n",
    "    path = os.path.join(coralme_dir, 'data', 'default_model_exchanges.pkl')\n",
    "    if os.path.exists(path):\n",
    "        pickle_in = open(path, 'rb')\n",
    "        default_exchanges = pickle.load(pickle_in)\n",
    "        pickle_in.close()\n",
    "    else:\n",
    "        default_exchanges = {}\n",
    "\n",
    "# create EC to pathway mapping\n",
    "resp = requests.get('https://rest.kegg.jp/link/ec/pathway')\n",
    "EC_to_pathways = {}\n",
    "for line in resp.text.split('\\n')[:-1]:\n",
    "    if 'path:ec' in line: continue\n",
    "    ec = line.split('\\t')[1][3:]\n",
    "    path = line.split('\\t')[0][5:]\n",
    "    if ec not in EC_to_pathways:\n",
    "        EC_to_pathways.update({ec : set()})\n",
    "    EC_to_pathways[ec].add(path)\n",
    "\n",
    "# now create mapping from pathway to name\n",
    "resp = requests.get('https://rest.kegg.jp/list/pathway')\n",
    "pathway_to_name = {}\n",
    "for line in resp.text.split('\\n')[:-1]:\n",
    "    pathway_to_name.update({line.split('\\t')[0] : line.split('\\t')[1]})\n",
    "\n",
    "# loop through each model\n",
    "spec_to_neg_EX = {}\n",
    "base_dir = os.path.join(coralme_dir, 'species_files', 'Pseudomonas_files')\n",
    "for f in os.listdir(os.path.join(base_dir, 'individual_species')):\n",
    "    if f == 'Reference': continue\n",
    "        \n",
    "    # load in ME_model\n",
    "    out_path = os.path.join(base_dir, 'individual_species', f, 'outputs')\n",
    "    if not force_rerun and os.path.exists(os.path.join(out_path, 'pathway_annotated_model.pkl')):\n",
    "        print(str(f)+' : pathway annotated model already created, skipping')\n",
    "        continue\n",
    "    out_dict = {}\n",
    "    opts = os.listdir(out_path)\n",
    "    model_name = None\n",
    "    for opt in opts:\n",
    "        if 'step3' in opt:\n",
    "            model_name = opt\n",
    "    if not model_name:\n",
    "        print(str(f)+' : model doesn\\'t exist')\n",
    "        continue\n",
    "    print(str(f)+' : loading model...', end = '')\n",
    "    ME_model = coralme.io.pickle.load_pickle_me_model(os.path.join(out_path, model_name))\n",
    "    model_path = os.path.join(base_dir, 'individual_species', f, 'inputs', 'model.json')\n",
    "    M_model = cobra.io.load_json_model(model_path)\n",
    "    \n",
    "    # look through exchange reactions to figure out what the media type is\n",
    "    print('finding media exchanges...', end = '')\n",
    "    spec_to_neg_EX.update({f : set()})\n",
    "    for rxn in ME_model.reactions:\n",
    "        if 'EX_' not in rxn.id:\n",
    "            continue\n",
    "        elif rxn.lower_bound < 0:\n",
    "            spec_to_neg_EX[f].add((rxn.id, rxn.lower_bound, rxn.upper_bound))\n",
    "\n",
    "    # let's also create subsystem annotations in the ME model\n",
    "    print('mapping pathways...', end = '')\n",
    "    mapping = create_m_to_me_mapping(M_model, ME_model)\n",
    "    mapping_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'M_to_ME_mapping.pkl')\n",
    "    pickle_out = open(mapping_path, 'wb')\n",
    "    pickle.dump(mapping, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    for M_rxn, ME_rxns in mapping.items():\n",
    "        if 'ec-code' not in M_model.reactions.get_by_id(M_rxn).annotation: continue\n",
    "        pathways = set()\n",
    "        for ec_code in M_model.reactions.get_by_id(M_rxn).annotation['ec-code']:\n",
    "            if ec_code not in EC_to_pathways: continue\n",
    "            pathways = pathways.union(EC_to_pathways[ec_code])\n",
    "        pathways = list(pathways)\n",
    "        pathways.sort()\n",
    "        for rxn in ME_rxns:\n",
    "            ME_model.reactions.get_by_id(rxn).annotation.update({'EC_id_pathways' : pathways})\n",
    "\n",
    "    # let's also create subsystem annotations in the ME model using the reference organisms\n",
    "    for M_rxn, ME_rxns in mapping.items():\n",
    "        if M_rxn not in models_source_df.index: continue\n",
    "        sub = models_source_df.loc[M_rxn].Subsystem\n",
    "        source = models_source_df.loc[M_rxn].source \n",
    "        for ME_rxn in ME_rxns:\n",
    "            ME_rxn = ME_model.reactions.get_by_id(ME_rxn)\n",
    "            ME_rxn.annotation.update({'ref_pathways' : sub})\n",
    "            ME_rxn.annotation.update({'ref_pathways_source' : source})\n",
    "    \n",
    "    # let's also pass along the GPR rules and add Carlos gene mapping for subsystems\n",
    "    for M_rxn, ME_rxns in mapping.items():\n",
    "        M_rxn = M_model.reactions.get_by_id(M_rxn)\n",
    "        for ME_rxn in ME_rxns:\n",
    "            ME_model.reactions.get_by_id(ME_rxn).annotation.update({'genes' : set(M_rxn.gpr.genes)})\n",
    "            subs = set()\n",
    "            for gene in M_rxn.gpr.genes:\n",
    "                check = models_carlos_df[models_carlos_df[f] == gene]\n",
    "                if len(check) == 0: continue\n",
    "                sub = check.Subsystem.values[0]\n",
    "                subs.add(sub)\n",
    "            subs = list(subs)\n",
    "            subs.sort()\n",
    "            ME_model.reactions.get_by_id(ME_rxn).annotation.update({'carlos_pathways' : subs})\n",
    "        \n",
    "    # now let's save the ME model with the annotations\n",
    "    print('saving model...', end = '')\n",
    "    coralme.io.pickle.save_pickle_me_model(ME_model, os.path.join(out_path, 'pathway_annotated_model.pkl'))\n",
    "    print(' done!')\n",
    "    if run_once:\n",
    "        break\n",
    "\n",
    "if False:\n",
    "    # save off changes\n",
    "    pickle_out = open(path, 'wb')\n",
    "    pickle.dump(default_exchanges, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961df80d-6869-4edb-be43-9ed80ce6023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP008896 : working...running M model..."
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Must pass valid ME-model json file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/zuniga/coralme/coralme/io/json.py:139\u001b[0m, in \u001b[0;36mload_json_me_model\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m \t\u001b[43mjsonschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m jsonschema\u001b[38;5;241m.\u001b[39mValidationError:\n",
      "File \u001b[0;32m~/miniforge3/envs/coralme/lib/python3.10/site-packages/jsonschema/validators.py:1332\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(instance, schema, cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[0;31mValidationError\u001b[0m: 'kt' is a required property\n\nFailed validating 'required' in schema['properties']['global_info']:\n    {'properties': {'GC_fraction': {'type': 'number'},\n                    'f_mRNA': {'type': 'number'},\n                    'f_rRNA': {'type': 'number'},\n                    'f_tRNA': {'type': 'number'},\n                    'k_deg': {'type': 'number'},\n                    'kt': {'type': 'number'},\n                    'm_aa': {'type': 'number'},\n                    'm_nt': {'type': 'number'},\n                    'm_rr': {'type': 'number'},\n                    'm_tRNA': {'type': 'number'},\n                    'propensity_scaling': {'type': 'number'},\n                    'r0': {'type': 'number'},\n                    'temperature': {'type': 'number'}},\n     'required': ['kt',\n                  'r0',\n                  'k_deg',\n                  'm_rr',\n                  'm_aa',\n                  'f_rRNA',\n                  'm_nt',\n                  'f_mRNA',\n                  'm_tRNA',\n                  'f_tRNA'],\n     'type': 'object'}\n\nOn instance['global_info']:\n    {'ME-Model-ID': None,\n     'START_tRNA': [],\n     'add_lipoproteins': False,\n     'add_translocases': True,\n     'amino_acid_loader': 'generic_Tuf',\n     'biomass_constraints': ['protein_biomass',\n                             'mRNA_biomass',\n                             'tRNA_biomass',\n                             'rRNA_biomass',\n                             'ncRNA_biomass',\n                             'tmRNA_biomass',\n                             'DNA_biomass',\n                             'lipid_biomass',\n                             'constituent_biomass',\n                             'prosthetic_group_biomass',\n                             'peptidoglycan_biomass'],\n     \"braun's_lipid_mod\": 'murein5px4p_p',\n     \"braun's_lipoproteins\": [],\n     \"braun's_lpp_flux\": -0.0,\n     \"braun's_murein_flux\": -0.0,\n     'compartments': {'c': 'Cytoplasm',\n                      'e': 'Extracellular',\n                      'mc': 'ME-model Constraint',\n                      'p': 'Periplasm'},\n     'complex_cofactors': {'FeFe/NiFe': {'mod_FeFe_cofactor_c': '',\n                                         'mod_NiFe_cofactor_c': ''},\n                           'biotin_subreactions': {'mod_btn_c': ['biotin_ligase']},\n                           'bmocogdp_chaperones': {},\n                           'fes_chaperones': {},\n                           'fes_transfers': [],\n                           'lipoate_subreactions': {'mod_lipoyl_c': ['lipoyl_denovo',\n                                                                     'lipoyl_scavenging']}},\n     'default_parameters': {'b': 0.1168587392731988,\n                            'd': 3.903641432780327,\n                            'f_mRNA': 0.02,\n                            'f_rRNA': 0.86,\n                            'f_tRNA': 0.12,\n                            'g_p_gdw_0': 0.059314110730022594,\n                            'g_per_gdw_inf': 0.02087208296776481,\n                            'k^default_cat': 65.0,\n                            'k^mRNA_deg': 12.0,\n                            'k_t': 4.5,\n                            'm_aa': 0.109,\n                            'm_nt': 0.324,\n                            'm_rr': 1453.0,\n                            'm_tRNA': 25.0,\n                            'propensity_scaling': 0.45,\n                            'r_0': 0.087,\n                            'temperature': 37.0},\n     'degradosome_id': 'RNA_degradosome',\n     'dnapol_id': 'DNAP',\n     'domain': 'Prokaryote',\n     'dummy_rxn_id': 'dummy_reaction',\n     'excision_machinery': ['rRNA_containing',\n                            'monocistronic',\n                            'polycistronic_wout_rRNA'],\n     'feature_types': ['CDS', 'rRNA', 'tRNA', 'ncRNA', 'tmRNA', 'misc_RNA'],\n     'gam': 34.98,\n     'genetic_recoding': {},\n     'genome_mods': {},\n     'growth_key': 'mu',\n     'include_pseudo_genes': False,\n     'knockouts': [],\n     'mg2_per_ribosome': 171,\n     'ngam': 1.0,\n     'peptide_processing_subreactions': ['Translation_termination_peptide_deformylase_processing',\n                                         'Translation_termination_peptide_chain_release',\n                                         'Translation_termination_ribosome_recycler'],\n     'peptide_release_factors': {'UAA': 'generic_RF',\n                                 'UAG': 'PrfA_mono',\n                                 'UGA': 'PrfB_mono'},\n     'ribosome_id': 'ribosome',\n     'rna_components': [],\n     'run_bbh_blast': True,\n     'transcription_subreactions': {'Transcription_normal_rho_dependent': 'atp_hydrolysis_rho',\n                                    'Transcription_normal_rho_independent': '',\n                                    'Transcription_stable_rho_dependent': 'atp_hydrolysis_rho',\n                                    'Transcription_stable_rho_independent': ''},\n     'translation_subreactions': {'Protein_processing_DnaK_dependent_folding': 'atp_hydrolysis',\n                                  'Protein_processing_GroEL_dependent_folding': 'atp_hydrolysis_groel',\n                                  'Protein_processing_N_terminal_methionine_cleavage': 'MAP',\n                                  'Ribosome_RbfA_mono_assembly_factor_phase1': '',\n                                  'Ribosome_RimM_mono_assembly_factor_phase1': '',\n                                  'Ribosome_gtp_bound_30S_assembly_factor_phase1': 'gtp_hydrolysis_era',\n                                  'Translation_elongation_FusA_mono': 'gtp_hydrolysis',\n                                  'Translation_elongation_Tuf_gtp_regeneration': '',\n                                  'Translation_initiation_factor_InfA': '',\n                                  'Translation_initiation_factor_InfC': '',\n                                  'Translation_initiation_fmet_addition_at_START': 'FMETTRS',\n                                  'Translation_initiation_gtp_factor_InfB': 'gtp_hydrolysis',\n                                  'Translation_termination_PrfA_mono_mediated': '',\n                                  'Translation_termination_PrfB_mono_mediated': '',\n                                  'Translation_termination_generic_RF_mediated': '',\n                                  'Translation_termination_peptide_chain_release': 'gtp_hydrolysis',\n                                  'Translation_termination_peptide_deformylase_processing': 'DEF',\n                                  'Translation_termination_ribosome_recycler': ''},\n     'translocation_pathway': {'bam': {'abbrev': 'b',\n                                       'keff': 0.027,\n                                       'length_dependent_energy': False,\n                                       'stoichiometry': ''},\n                               'lol': {'abbrev': 'l',\n                                       'keff': 0.9,\n                                       'length_dependent_energy': False,\n                                       'stoichiometry': 'atp_hydrolysis'},\n                               'sec': {'abbrev': 's',\n                                       'keff': 4.0,\n                                       'length_dependent_energy': True,\n                                       'stoichiometry': 'atp_hydrolysis_sec_pathway'},\n                               'secA': {'abbrev': 'a',\n                                        'keff': 4.0,\n                                        'length_dependent_energy': True,\n                                        'stoichiometry': 'atp_hydrolysis_secA'},\n                               'srp': {'FtsY': 'FtsY_MONOMER',\n                                       'abbrev': 'r',\n                                       'keff': 20.0,\n                                       'length_dependent_energy': False,\n                                       'stoichiometry': 'gtp_hydrolysis_srp_pathway'},\n                               'srp_yidC': {'abbrev': 'p',\n                                            'keff': 20.0,\n                                            'length_dependent_energy': False,\n                                            'stoichiometry': 'gtp_hydrolysis'},\n                               'tat': {'abbrev': 't',\n                                       'keff': 0.0125,\n                                       'length_dependent_energy': False,\n                                       'stoichiometry': ''},\n                               'tat_alt': {'abbrev': 't',\n                                           'keff': 0.0125,\n                                           'length_dependent_energy': False,\n                                           'stoichiometry': ''},\n                               'yidC': {'abbrev': 'y',\n                                        'keff': 20.0,\n                                        'length_dependent_energy': False,\n                                        'stoichiometry': 'gtp_hydrolysis'}},\n     'trna_misacylation': {},\n     'trna_to_aa': {},\n     'trna_to_codon': {},\n     'unmodeled_protein_fraction': 0.36}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \u001b[38;5;66;03m# zzz force_rerun or not os.path.exists(M_sol_path):\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning M model...\u001b[39m\u001b[38;5;124m'\u001b[39m, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     M_model \u001b[38;5;241m=\u001b[39m \u001b[43mcoralme\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_json_me_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM_v2_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# run on this default medium\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     solution \u001b[38;5;241m=\u001b[39m M_model\u001b[38;5;241m.\u001b[39moptimize(max_mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, min_mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m, tolerance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m, maxIter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[0;32m~/zuniga/coralme/coralme/io/json.py:141\u001b[0m, in \u001b[0;36mload_json_me_model\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m    139\u001b[0m \tjsonschema\u001b[38;5;241m.\u001b[39mvalidate(model_dict, get_schema())\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m jsonschema\u001b[38;5;241m.\u001b[39mValidationError:\n\u001b[0;32m--> 141\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMust pass valid ME-model json file\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    143\u001b[0m model \u001b[38;5;241m=\u001b[39m coralme\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mdict\u001b[38;5;241m.\u001b[39mme_model_from_dict(model_dict)\n\u001b[1;32m    145\u001b[0m logging\u001b[38;5;241m.\u001b[39mshutdown()\n",
      "\u001b[0;31mException\u001b[0m: Must pass valid ME-model json file"
     ]
    }
   ],
   "source": [
    "# check for solutions, run if nonexistant\n",
    "base_dir = os.path.join(coralme_dir, 'species_files', 'Pseudomonas_files')\n",
    "for f in os.listdir(os.path.join(base_dir, 'individual_species')):\n",
    "    if 'Reference' in f: continue\n",
    "    \n",
    "    # look to see if solution already exists\n",
    "    sol_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'flux_solution.pkl')\n",
    "    M_sol_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'M_flux_solution.pkl')\n",
    "    M_flux_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'M_flux_ref_solution_dict.pkl')\n",
    "    flux_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'flux_EC_solution_dict.pkl')\n",
    "    flux_path2 = os.path.join(base_dir, 'individual_species', f, 'outputs', 'flux_ref_solution_dict.pkl')\n",
    "    out_path = os.path.join(base_dir, 'individual_species', f, 'outputs')\n",
    "    if False: # zzz not force_rerun and os.path.exists(sol_path) and os.path.exists(M_sol_path) and os.path.exists(flux_path) and os.path.exists(flux_path2):\n",
    "        print(str(f)+' : already run')\n",
    "        continue\n",
    "    if not os.path.exists(os.path.join(out_path, 'pathway_annotated_model.pkl')):\n",
    "        print(str(f)+' : pathway annotated model does not exist, skipping')\n",
    "        continue\n",
    "    print(str(f)+' : working...', end = '')\n",
    "    \n",
    "    # run M model simulations\n",
    "    M_model_path = os.path.join(base_dir, 'individual_species', f, 'inputs', 'model.json')\n",
    "    M_v2_model_path = os.path.join(base_dir, 'individual_species', f, 'inputs', 'M_as_ME_for_QMINOS_model.json')\n",
    "    if True: # zzz force_rerun or not os.path.exists(M_sol_path):\n",
    "        print('running M model...', end = '')\n",
    "        M_model = coralme.io.json.load_json_me_model(M_v2_model_path)\n",
    "\n",
    "        # run on this default medium\n",
    "        solution = M_model.optimize(max_mu = 0.5, min_mu = 0.01, tolerance = 1e-4, maxIter = 20)\n",
    "    \n",
    "        # if there is a solution, create pathway to flux and annotation, save them\n",
    "        if solution:\n",
    "            print('saving solution...', end = '')\n",
    "            rxns = []\n",
    "            fluxes = []\n",
    "            pathways = []\n",
    "            for rxn, flux in M_model.solution.fluxes.items():\n",
    "                M_rxn = M_model.reactions.get_by_id(rxn)\n",
    "                if 'ref_pathways' in M_rxn.annotation:\n",
    "                    rxns.append(rxn)\n",
    "                    fluxes.append(flux)\n",
    "                    pathways.append(M_rxn.annotation['ref_pathways'])\n",
    "            flux_df = pd.DataFrame(index = rxns)\n",
    "            flux_df['flux'] = fluxes\n",
    "            flux_df['pathways'] = pathways\n",
    "            flux_df.to_pickle(M_flux_path)\n",
    "        \n",
    "            # save off solution\n",
    "            pickle_out = open(M_sol_path, 'wb')\n",
    "            pickle.dump(ME_model.solution, pickle_out)\n",
    "            pickle_out.close()\n",
    "        else:\n",
    "            print('no solution...', end = '')\n",
    "            # save something bogus to prevent rerunning this\n",
    "            pickle_out = open(M_flux_path, 'wb')\n",
    "            pickle.dump(False, pickle_out)\n",
    "            pickle_out.close()\n",
    "            \n",
    "            # save off solution\n",
    "            pickle_out = open(M_sol_path, 'wb')\n",
    "            pickle.dump(False, pickle_out)\n",
    "            pickle_out.close()\n",
    "    \n",
    "    # run ME model simulations if it doesn't exist\n",
    "    if force_rerun or not os.path.exists(sol_path):\n",
    "        print('running ME model...', end = '')\n",
    "        ME_model = coralme.io.pickle.load_pickle_me_model(os.path.join(out_path, 'pathway_annotated_model.pkl'))\n",
    "        \n",
    "        # run on this default medium\n",
    "        solution = ME_model.optimize(max_mu = 0.5, min_mu = 0.01, tolerance = 1e-4, maxIter = 20)\n",
    "    \n",
    "        # if there is a solution, create pathway to flux and annotation, save them\n",
    "        if solution:\n",
    "            print('saving solution...', end = '')\n",
    "            rxns = []\n",
    "            fluxes = []\n",
    "            pathways = []\n",
    "            rxns2 = []\n",
    "            fluxes2 = []\n",
    "            pathways2 = []\n",
    "            for rxn, flux in ME_model.solution.fluxes.items():\n",
    "                ME_rxn = ME_model.reactions.get_by_id(rxn)\n",
    "                if 'EC_id_pathways' in ME_rxn.annotation:\n",
    "                    rxns.append(rxn)\n",
    "                    fluxes.append(flux)\n",
    "                    pathways.append(ME_rxn.annotation['EC_id_pathways'])\n",
    "                if 'ref_pathways' in ME_rxn.annotation:\n",
    "                    rxns2.append(rxn)\n",
    "                    fluxes2.append(flux)\n",
    "                    pathways2.append(ME_rxn.annotation['ref_pathways'])\n",
    "            flux_df = pd.DataFrame(index = rxns)\n",
    "            flux_df['flux'] = fluxes\n",
    "            flux_df['pathways'] = pathways\n",
    "            flux_df.to_pickle(flux_path)\n",
    "            flux_df2 = pd.DataFrame(index = rxns2)\n",
    "            flux_df2['flux'] = fluxes2\n",
    "            flux_df2['pathways'] = pathways2\n",
    "            flux_df2.to_pickle(flux_path2)\n",
    "        \n",
    "            # save off solution\n",
    "            pickle_out = open(sol_path, 'wb')\n",
    "            pickle.dump(ME_model.solution, pickle_out)\n",
    "            pickle_out.close()\n",
    "        else:\n",
    "            print('no solution...', end = '')\n",
    "            # save something bogus to prevent rerunning this\n",
    "            pickle_out = open(flux_path, 'wb')\n",
    "            pickle.dump(False, pickle_out)\n",
    "            pickle_out.close()\n",
    "            pickle_out = open(flux_path2, 'wb')\n",
    "            pickle.dump(False, pickle_out)\n",
    "            pickle_out.close()\n",
    "            \n",
    "            # save off solution\n",
    "            pickle_out = open(sol_path, 'wb')\n",
    "            pickle.dump(False, pickle_out)\n",
    "            pickle_out.close()\n",
    "    print(' done!')\n",
    "    if run_once:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86eeab-4cf2-4d67-a33b-dc10469d9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(coralme.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be1c32-da10-488f-b673-7b0b9faf9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_as_ME_model.global_info.update({\n",
    "    'kt': 4.5,  # This was k_t in your default_parameters\n",
    "    'r0': 0.087,\n",
    "    'k_deg': 12.0,  # This was k^mRNA_deg in your default_parameters\n",
    "    'm_rr': 1453.0,\n",
    "    'm_aa': 0.109,\n",
    "    'f_rRNA': 0.86,\n",
    "    'm_nt': 0.324,\n",
    "    'f_mRNA': 0.02,\n",
    "    'm_tRNA': 25.0,\n",
    "    'f_tRNA': 0.12\n",
    "})\n",
    "coralme.io.json.save_json_me_model(M_as_ME_model, M_v2_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec90af0-735f-4714-9918-31e0d63d8884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "opts = list(set(models_carlos_df.Subsystem))\n",
    "\n",
    "# let's loop through all of these to pull out flux_dfs\n",
    "base_dir = os.path.join(coralme_dir, 'species_files', 'Pseudomonas_files')\n",
    "plt_df = pd.DataFrame(index = opts)\n",
    "total_df = pd.DataFrame(index = opts)\n",
    "for f in os.listdir(os.path.join(base_dir, 'individual_species')):\n",
    "    if 'Reference' in f: continue\n",
    "    \n",
    "    # look to see if solution already exists\n",
    "    M_sol_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'M_flux_solution.pkl')\n",
    "    M_flux_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'M_flux_ref_solution_dict.pkl')\n",
    "    ME_sol_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'flux_solution.pkl')\n",
    "    ME_flux_path= os.path.join(base_dir, 'individual_species', f, 'outputs', 'flux_ref_solution_dict.pkl')\n",
    "    tot_paths = [M_sol_path, M_flux_path, ME_sol_path, ME_flux_path]\n",
    "    if not all([os.path.exists(path) for path in tot_paths]):# not os.path.exists(sol_path) or not os.path.exists(M_sol_path) or not os.path.exists(flux_path) or not os.path.exists(flux_path2):\n",
    "        print(str(f)+' : not yet run')\n",
    "        continue\n",
    "    \n",
    "    # load in results\n",
    "    pickle_in = open(M_sol_path, 'rb')\n",
    "    M_sol = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    pickle_in = open(M_flux_path, 'rb')\n",
    "    M_flux_df = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    pickle_in = open(ME_sol_path, 'rb')\n",
    "    ME_sol = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    pickle_in = open(ME_flux_path, 'rb')\n",
    "    ME_flux_df = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    mapping_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'M_to_ME_mapping.pkl')\n",
    "    pickle_in = open(mapping_path, 'rb')\n",
    "    M_to_ME = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "\n",
    "    # check if solutions exist\n",
    "    if not ME_sol or not M_sol:\n",
    "        print(str(f)+' : no solution, skipping')\n",
    "        continue\n",
    "    \n",
    "    # create combined Dataframe standardized by growth rates\n",
    "    rxns = []\n",
    "    M_fluxes = []\n",
    "    ME_sum_fluxes = []\n",
    "    ME_abs_sum_fluxes = []\n",
    "    pathways = []\n",
    "    for M_rxn, ME_rxns in M_to_ME.items():\n",
    "        if not all([rxn in ME_flux_df.index for rxn in ME_rxns]):\n",
    "            continue\n",
    "        overlap = [rxn for rxn in ME_rxns if rxn in ME_flux_df.index]\n",
    "        M_flux = M_flux_df.loc[M_rxn]['flux'] / M_sol.objective_value\n",
    "        ME_flux = ME_flux_df.loc[overlap]['flux'] / ME_sol.objective_value\n",
    "        for pathway in M_flux_df.loc[M_rxn]['pathways']:\n",
    "            rxns.append(M_rxn)\n",
    "            M_fluxes.append(M_flux)\n",
    "            ME_sum_fluxes.append(sum(ME_flux.values))\n",
    "            ME_abs_sum_fluxes.append(sum(abs(ME_flux.values)))\n",
    "            pathways.append(pathway)\n",
    "    summed_df = pd.DataFrame(index = rxns)\n",
    "    summed_df['M_fluxes'] = M_fluxes\n",
    "    summed_df['sum_abs_ME_fluxes'] = ME_abs_sum_fluxes\n",
    "    summed_df['sum_ME_fluxes'] = ME_sum_fluxes\n",
    "    summed_df['pathways'] = pathways\n",
    "\n",
    "    # now convert to pathway summed flux for plotting\n",
    "    # copying from https://www.nature.com/articles/s41467-020-17612-8\n",
    "    groups = []\n",
    "    M_scores = []\n",
    "    ME_scores = []\n",
    "    for group, df in summed_df.groupby('pathways'):\n",
    "        if sum(abs(df['M_fluxes'])) == 0:\n",
    "            M_score = 0\n",
    "        else:\n",
    "            M_score = sum(df['M_fluxes']) / sum(abs(df['M_fluxes']))\n",
    "        if sum(df['sum_abs_ME_fluxes']) == 0:\n",
    "            ME_score = 0\n",
    "        else:\n",
    "            ME_score = sum(df['sum_ME_fluxes']) / sum(df['sum_abs_ME_fluxes'])\n",
    "        groups.append(group)\n",
    "        M_scores.append(M_score)\n",
    "        ME_scores.append(ME_score)\n",
    "    plot_df = pd.DataFrame(index = groups)\n",
    "    plot_df['M_score'] = M_scores\n",
    "    plot_df['ME_score'] = ME_scores\n",
    "    plot_df[f] = plot_df['ME_score'] - plot_df['M_score']\n",
    "    plt_df = plt_df.join(plot_df[f], how='left')\n",
    "    total_df = total_df.join(plot_df.rename(columns = {'M_score' : 'M_'+f, 'ME_score' : 'ME_'+f}), how = 'left')\n",
    "\n",
    "    ################################################\n",
    "    # individual plot #\n",
    "    ################################################\n",
    "    diff_groups = plot_df.index[plot_df['M_score'] != plot_df['ME_score']]\n",
    "    show_df = plot_df.loc[diff_groups]\n",
    "    x_vals = [i for i in range(len(show_df.index))]\n",
    "    plt.scatter(x_vals, show_df['M_score'], c = 'red', label = 'M model')\n",
    "    plt.scatter(x_vals, show_df['ME_score'], c = 'blue', label = 'ME model')\n",
    "    plt.xticks(np.arange(0, max(x_vals)+1), show_df.index, rotation = 90)\n",
    "    plt.xlabel('Pathways')\n",
    "    plt.ylabel('Flux by Pathway (mmol/gDW/h)')\n",
    "    plt.title('Strain: '+f)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "    plt.savefig('../figures/strain_figures/'+f+'_flux_figure.pdf', transparent = True, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "total_df.to_csv('../data/summed_flux_df.csv')\n",
    "\n",
    "################################################\n",
    "# overall plot #\n",
    "################################################\n",
    "plt_df = plt_df.dropna(0, how = 'all').fillna(0)\n",
    "keep = [any([val not in [-1, 0, 1] for val in vals]) for vals in plt_df.values] # remove samples that contain only -1s, 0s, and 1s\n",
    "plt_df = plt_df.loc[keep].sort_index()\n",
    "\n",
    "colors = get_n_colors_from_colormap(len(plt_df.columns), 'plasma')\n",
    "col_to_colors = {col : color for col, color in zip(plt_df.columns, colors)}\n",
    "colors = []\n",
    "x_vals = []\n",
    "y_vals = []\n",
    "labels = []\n",
    "labels_long = []\n",
    "i = 0\n",
    "for index in plt_df.index:\n",
    "    labels.append(index)\n",
    "    for col in plt_df.columns:\n",
    "        val = plt_df.loc[index][col]\n",
    "        x_vals.append(i)\n",
    "        y_vals.append(val)\n",
    "        labels_long.append(col)\n",
    "        colors.append(col_to_colors[col])\n",
    "    i += 1\n",
    "fig, ax = plt.subplots()\n",
    "sorted_x = list(set(x_vals))\n",
    "sorted_x.sort()\n",
    "for x in sorted_x:\n",
    "    plt.axvline(x = x, c = 'grey', alpha = 0.2)\n",
    "plt.scatter(x_vals, y_vals, c = colors)\n",
    "plt.xticks(np.arange(0, max(x_vals)+1), labels, rotation = 90)\n",
    "plt.xlabel('Pathways')\n",
    "plt.ylabel('ME - M Model Flux by Pathway (mmol/gDW/h)')\n",
    "\n",
    "# label the \"outliers\"\n",
    "x_labs = []\n",
    "y_labs = []\n",
    "labs = []\n",
    "for x_val, y_val, lab_temp in zip(x_vals, y_vals, labels_long):\n",
    "    if abs(y_val) > 0.05:\n",
    "        x_labs.append(x_val)\n",
    "        y_labs.append(y_val)\n",
    "        labs.append(lab_temp)\n",
    "if len(labs) < 50:\n",
    "    texts = [ax.text(x_labs[i], y_labs[i], labs[i], ha='center', va='center') for i in range(len(x_labs))]\n",
    "    _ = adjust_text(texts, expand=(1.2, 1.2), # expand text bounding boxes by 1.2 fold in x direction and 2 fold in y direction\n",
    "                arrowprops=dict(arrowstyle='->', color='k')); # ensure the labeling is clear by adding arrows);\n",
    "\n",
    "# Custom legend using patches\n",
    "legend_patches = [mpatches.Patch(color=color, label=label) for label, color in col_to_colors.items()]\n",
    "plt.legend(handles=legend_patches, title='Species', loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.savefig(os.path.join(coralme_dir, 'figures', 'pathway_flux.pdf'), transparent = True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# most commonly outlier\n",
    "print('How often labelled:')\n",
    "label_to_ct = dict(Counter(labs))\n",
    "sorted_labels = [k for k, _ in sorted(label_to_ct.items(), key = lambda k : -k[1])]\n",
    "for k in sorted_labels:\n",
    "    print(str(k)+' : '+str(label_to_ct[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e1f050-cfa2-4c6a-8435-23494d31e977",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce37e1bf-5cf8-4ddc-9792-037e67d9de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "use_flux = 1\n",
    "pathways_df = pd.DataFrame(index = pathway_to_name.keys())\n",
    "\n",
    "# let's loop through all of these to pull out flux_dfs\n",
    "base_dir = os.path.join(coralme_dir, 'species_files', 'Pseudomonas_files')\n",
    "for f in os.listdir(os.path.join(base_dir, 'individual_species')):\n",
    "    if 'Reference' in f: continue\n",
    "    \n",
    "    # look to see if solution already exists\n",
    "    sol_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'flux_solution.pkl')\n",
    "    M_sol_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'M_flux_solution.pkl')\n",
    "    M_flux_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'M_flux_ref_solution_dict.pkl')\n",
    "    flux_path = os.path.join(base_dir, 'individual_species', f, 'outputs', 'flux_EC_solution_dict.pkl')\n",
    "    flux_path2 = os.path.join(base_dir, 'individual_species', f, 'outputs', 'flux_ref_solution_dict.pkl')\n",
    "    tot_paths = [sol_path, M_sol_path, M_flux_path, flux_path, flux_path2]\n",
    "    if not all([os.path.exists(path) for path in tot_paths]):# not os.path.exists(sol_path) or not os.path.exists(M_sol_path) or not os.path.exists(flux_path) or not os.path.exists(flux_path2):\n",
    "        print(str(f)+' : not yet run')\n",
    "        continue\n",
    "    \n",
    "    # load in results\n",
    "    pickle_in = open(sol_path, 'rb')\n",
    "    sol = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    pickle_in = open(flux_path, 'rb')\n",
    "    flux_df = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    pickle_in = open(flux_path2, 'rb')\n",
    "    flux_df2 = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    if type(flux_df) == bool or len(flux_df) == 0:\n",
    "        continue\n",
    "    \n",
    "    # reorganize by pathway groups\n",
    "    pathway_to_fluxes = {}\n",
    "    if use_flux == 1:\n",
    "        for index, row in flux_df.iterrows():\n",
    "            pathways = row['pathways']\n",
    "            if str(pathways) == 'nan': continue\n",
    "            for pathway in pathways:\n",
    "                if pathway not in pathway_to_fluxes:\n",
    "                    pathway_to_fluxes.update({pathway : []})\n",
    "                pathway_to_fluxes[pathway].append(row['flux'])\n",
    "    else:\n",
    "        for index, row in flux_df2.iterrows():\n",
    "            pathway = row['pathways']\n",
    "            if str(pathways) == 'nan': continue\n",
    "            if pathway not in pathway_to_fluxes:\n",
    "                pathway_to_fluxes.update({pathway : []})\n",
    "            pathway_to_fluxes[pathway].append(row['flux'])\n",
    "    \n",
    "    # take max of each to represent the flux through the map (not sure what the best strategy is, maybe manually pick bottlenecks?)\n",
    "    vals = []\n",
    "    for pathway in pathways_df.index:\n",
    "        if pathway in pathway_to_fluxes:\n",
    "            # copying from https://www.nature.com/articles/s41467-020-17612-8\n",
    "            total = sum(pathway_to_fluxes[pathway])\n",
    "            abs_total = sum([abs(val) for val in pathway_to_fluxes[pathway]])\n",
    "            if abs_total == 0:\n",
    "                vals.append(0)\n",
    "            else:\n",
    "                vals.append(total / abs_total)\n",
    "        else:\n",
    "            vals.append(None)\n",
    "    pathways_df[f] = vals\n",
    "\n",
    "# now we can plot\n",
    "pathways_df = pathways_df.rename(pathway_to_name).sort_index(key = lambda ind : ind.str.lower())\n",
    "pathways_df = pathways_df.dropna(0, how = 'all')\n",
    "keep = pathways_df.index[abs(pathways_df).sum(axis = 1) != 0] # remove examples where all zero\n",
    "pathways_df = pathways_df.loc[keep]\n",
    "keep = pathways_df.index[abs(pathways_df).sum(axis = 1) != len(pathways_df.columns)] # remove examples of all 1s or all -1s\n",
    "pathways_df = pathways_df.loc[keep]\n",
    "keep = [any([val not in [-1, 0, 1] for val in vals]) for vals in pathways_df.values] # remove samples that contain only -1s, 0s, and 1s\n",
    "pathways_df = pathways_df.loc[keep]\n",
    "\n",
    "colors = get_n_colors_from_colormap(len(pathways_df.columns), 'plasma')\n",
    "col_to_colors = {col : color for col, color in zip(pathways_df.columns, colors)}\n",
    "colors = []\n",
    "x_vals = []\n",
    "y_vals = []\n",
    "labels = []\n",
    "labels_long = []\n",
    "i = 0\n",
    "for index in pathways_df.index:\n",
    "    labels.append(index)\n",
    "    for col in pathways_df.columns:\n",
    "        val = pathways_df.loc[index][col]\n",
    "        x_vals.append(i)\n",
    "        y_vals.append(val)\n",
    "        labels_long.append(col)\n",
    "        colors.append(col_to_colors[col])\n",
    "    i += 1\n",
    "fig, ax = plt.subplots()\n",
    "sorted_x = list(set(x_vals))\n",
    "sorted_x.sort()\n",
    "for x in sorted_x:\n",
    "    plt.axvline(x = x, c = 'grey', alpha = 0.2)\n",
    "plt.scatter(x_vals, y_vals, c = colors)\n",
    "plt.xticks(np.arange(0, max(x_vals)+1), labels, rotation = 90)\n",
    "plt.xlabel('Pathways')\n",
    "plt.ylabel('Flux by Pathway (mmol/gDW/h)')\n",
    "\n",
    "# label the \"outliers\"\n",
    "x_labs = []\n",
    "y_labs = []\n",
    "labs = []\n",
    "for x_val, y_val, lab_temp in zip(x_vals, y_vals, labels_long):\n",
    "    if abs(y_val) < 0.95 and abs(y_val) > 0.05:\n",
    "        x_labs.append(x_val)\n",
    "        y_labs.append(y_val)\n",
    "        labs.append(lab_temp)\n",
    "texts = [ax.text(x_labs[i], y_labs[i], labs[i], ha='center', va='center') for i in range(len(x_labs))]\n",
    "_ = adjust_text(texts, expand=(1.2, 1.2), # expand text bounding boxes by 1.2 fold in x direction and 2 fold in y direction\n",
    "            arrowprops=dict(arrowstyle='->', color='k')); # ensure the labeling is clear by adding arrows);\n",
    "\n",
    "# Custom legend using patches\n",
    "legend_patches = [mpatches.Patch(color=color, label=label) for label, color in col_to_colors.items()]\n",
    "plt.legend(handles=legend_patches, title='Species', loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.savefig(os.path.join(coralme_dir, 'figures', 'pathway_flux.pdf'), transparent = True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# most commonly outlier\n",
    "print('How often labelled:')\n",
    "label_to_ct = dict(Counter(labs))\n",
    "sorted_labels = [k for k, _ in sorted(label_to_ct.items(), key = lambda k : -k[1])]\n",
    "for k in sorted_labels:\n",
    "    print(str(k)+' : '+str(label_to_ct[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985711ef-971f-4fe5-b4a5-e04e676bfe00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
